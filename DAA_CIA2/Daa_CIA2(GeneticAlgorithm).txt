First, we need to define the population of individuals, which are solutions to the problem represented as binary strings. Each bit in the string corresponds to a feature in the dataset, where 1 indicates that the feature is included in the model and 0 indicates that it is excluded.

import random

# Define the size of the population and the length of each individual
population_size = 100
individual_length = len(features)

# Generate a random population of individuals
population = []
for i in range(population_size):
    individual = [random.randint(0, 1) for j in range(individual_length)]
    population.append(individual)

Next, we need to define the fitness function, which evaluates the performance of each individual on the problem. For this problem, we can use the accuracy of the model as the fitness function.

def fitness(individual):
    # Convert the binary string to a list of selected features
    selected_features = [features[i] for i in range(len(individual)) if individual[i] == 1]
    
    # Train and evaluate a logistic regression model using the selected features
    X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.2)
    model = LogisticRegression()
    model.fit(X_train, y_train)
    accuracy = model.score(X_test, y_test)
    
    return accuracy

Once we have the population and fitness function, we can use the genetic algorithm to evolve the population over multiple generations.

# Define the parameters of the genetic algorithm
mutation_rate = 0.01
elite_size = 10
generations = 100

# Evolve the population over multiple generations
for generation in range(generations):
    # Evaluate the fitness of each individual
    fitness_scores = [(individual, fitness(individual)) for individual in population]
    
    # Sort the individuals by fitness score in descending order
    fitness_scores.sort(key=lambda x: x[1], reverse=True)
    
    # Select the elite individuals to carry over to the next generation
    elite = [individual for individual, score in fitness_scores[:elite_size]]
    
    # Breed new individuals by crossover and mutation
    offspring = []
    while len(offspring) < population_size - elite_size:
        # Select two parents from the population
        parent1, parent2 = random.choices(population, weights=[score for individual, score in fitness_scores], k=2)
        
        # Perform crossover to create a new offspring
        crossover_point = random.randint(1, individual_length - 1)
        offspring1 = parent1[:crossover_point] + parent2[crossover_point:]
        offspring2 = parent2[:crossover_point] + parent1[crossover_point:]
        
        # Mutate the offspring by flipping a random bit
        if random.random() < mutation_rate:
            mutation_point = random.randint(0, individual_length - 1)
            offspring1[mutation_point] = 1 - offspring1[mutation_point]
            offspring2[mutation_point] = 1 - offspring2[mutation_point]
            
        offspring.append(offspring1)
        offspring.append(offspring2)
    
    # Combine the elite and offspring to form the next generation
    population = elite + offspring
